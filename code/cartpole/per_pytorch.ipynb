{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Experience Replay\n",
    "- agent는 기존 DQN/Double DQN과 동일\n",
    "- 다만, replay buffer 저장하고 sampling하는 방식에서 차이를 보인다.\n",
    "- 어떻게 저장되는 transition의 우선순위를 정할 것인가에 집중하여 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape :  (4,)\n",
      "# of action :  2\n"
     ]
    }
   ],
   "source": [
    "# state, action shape 확인\n",
    "print(\"state shape : \",env.observation_space.shape)\n",
    "print(\"# of action : \",env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 셋팅은 DQN과 동일\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2000\n",
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped # ??\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "# isinstance : action_space의 sample action의 type이 int인가? True or False return\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network setting\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.l1 = nn.Linear(N_STATES, 24)\n",
    "        self.l1.weight.data.normal_(0, 0.1)\n",
    "        self.l2 = nn.Linear(24, 24)\n",
    "        self.l2.weight.data.normal_(0 ,0.1)\n",
    "        self.out = nn.Linear(24, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0, 0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Ready for Training!!\n"
     ]
    }
   ],
   "source": [
    "# agent setting\n",
    "class DQN:\n",
    "    def __init__(self):\n",
    "        # 먼저 prediction network와 target network 분리\n",
    "        self.pred_net, self.target_net = Net(), Net()\n",
    "        \n",
    "        self.learn_step_counter = 0 # for target updating\n",
    "        self.memory_counter = 0 # for storing memory\n",
    "        # (capacity, 4) ; 4 = (state, action, reward, next_state)\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))\n",
    "        self.optimizer = torch.optim.Adam(self.pred_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss() # 이러면 알아서 y와 y_hat을 인식하나??\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), dim=0) # (1, ?)\n",
    "        # 1. greedy \n",
    "        if np.random.uniform() < EPSILON: # EPSILON = 0.9\n",
    "            actions_value = self.pred_net.forward(x)\n",
    "            # [1]은 return값중 argmax를 받겠다는 의미\n",
    "            # max의 return값이 max, argmax 두개가 나옴\n",
    "            # .data까지는 print해주는 output의 변화없고 이걸 numpy로 변환\n",
    "            action = torch.max(actions_value, dim=1)[1].data.numpy()\n",
    "            # 아래가 뭘 의미하는 거지...\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        \n",
    "        # random\n",
    "        else:\n",
    "            action = np.random.randint(0, N_ACTIONS) # 0,1중에 하나 선택\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        # state가 list/array이기때문에, action과 reward를 []로 묶어준다.\n",
    "        # **state가 4가지 정보를 담고있다.\n",
    "        transition = np.hstack([state, [action,reward], next_state])\n",
    "        # replace the old memory with new memory\n",
    "        idx = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[idx,:] = transition\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "    def learn(self):\n",
    "        # TARGET_REPLACE_ITER은 100으로 설정했다.\n",
    "        # 이 말은 즉 target network를 100번에 한번씩 업데이트하겠다는 의미\n",
    "        # pred_net 100번 업데이트하고 나서야 비로소 target_net 1번 업데이트\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            # target의 parameter를 pred_net의 parameter로 대체하겠다는 의미\n",
    "            self.target_net.load_state_dict(self.pred_net.state_dict())\n",
    "            \n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        # batch transition sampling\n",
    "        sample_idx = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_idx, :]\n",
    "        b_state = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_action = torch.LongTensor(b_memory[:, N_STATES : N_STATES+1].astype(int)) # long 이어야함 (Integer)\n",
    "        b_reward = torch.FloatTensor(b_memory[:, N_STATES+1 : N_STATES+2])\n",
    "        b_next_state = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        q_pred = self.pred_net(b_state).gather(1, b_action) # gather의 의미를 모르겠다..\n",
    "        q_prime = self.target_net(b_next_state).detach() # detach from graph, don't backpropagate(gradient가 흘러들어가 업데이트 되지않도록 예방)\n",
    "        q_target = b_reward + GAMMA * q_prime.max(dim=1)[0].view(BATCH_SIZE,1) # shape (batch, 1)\n",
    "        loss = self.loss_func(q_pred, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad() # gradient initialize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "print(\"Get Ready for Training!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prioritized replay memory \n",
    "# class PER:\n",
    "#     def __init__(self, alpha, beta):\n",
    "#         self.alpha = alpha # alpha in prioritization computation\n",
    "#         self.beta = beta # beta in importance sampling weight computation\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prioritized Experience Replay\n",
    "- 주찬웅님 github 발췌 : [주찬웅님 github링크](https://github.com/rlcode/per/blob/master/SumTree.py)\n",
    "- replay memory를 어떻게 정렬할 것인가?\n",
    "- heap sort(힙 정렬)의 개념이 들어감 : [heap sort 개념링크](https://github.com/Junhojuno/Lets-RL/blob/master/code/cartpole/heapSort.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    write = 0\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # capacity는 memory_size를 의미\n",
    "        \n",
    "        # 1D array (완전이진트리)\n",
    "        # 왜 2*capacity - 1인가? --> 아마 정렬시 root node와 leaf node를 맞바꾸기 위함인가? 미리 늘려주는건가?\n",
    "        self.tree = np.zeros(2 * capacity - 1) \n",
    "        \n",
    "        # 이건 뭐지?\n",
    "        self.data = np.zeros(capacity, dtype=object)\n",
    "        \n",
    "        # entries setting (initialize)\n",
    "        self.n_entries = 0\n",
    "\n",
    "    # update to the root node\n",
    "    # root index까지 key값을 더한다.\n",
    "    # 그리고 해당 idx의 부모노드에 idx의 p값이 변화한 정도를 반영해주는 함수다.\n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2 # parent node 구하려면 해당 노드를 2로 나눠주는건데 -1은 왜 한거지?\n",
    "\n",
    "        self.tree[parent] += change\n",
    "\n",
    "        if parent != 0: # root node가 아니면 다시 parent node구한다.\n",
    "            self._propagate(parent, change)\n",
    "\n",
    "    # find sample on leaf node\n",
    "    # leaf index를 뽑는다.\n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = 2 * idx + 2 # left + 1과 동일한 값\n",
    "\n",
    "        if left >= len(self.tree): # left index값이 dataset의 길이보다 크거나 같으면\n",
    "            return idx\n",
    "\n",
    "        if s <= self.tree[left]:\n",
    "            return self._retrieve(left, s)\n",
    "        else: # s > tree[left]\n",
    "            return self._retrieve(right, s - self.tree[left])\n",
    "\n",
    "    # total이 왜 tree[0]값이지?\n",
    "    # total이라는게 TD error의 총값이고 그게 node의 key값이고\n",
    "    # 이 값은 root node의 key값이 된다.(그래서 위에서 += change해줌)\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "\n",
    "    # store priority and sample\n",
    "    def add(self, p, data):\n",
    "        idx = self.write + self.capacity - 1 # 첫 시작은 가장 마지막 인덱스 ex) 20000의 capacity라면 idx=19999\n",
    "\n",
    "        self.data[self.write] = data\n",
    "        self.update(idx, p)\n",
    "\n",
    "        self.write += 1\n",
    "        if self.write >= self.capacity:\n",
    "            self.write = 0\n",
    "\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "\n",
    "    # update priority\n",
    "    # p값이 바뀌면 해당 idx와 그 idx의 부모노드에 까지 변화량을 반영해주겠다는 의미\n",
    "    def update(self, idx, p): # 해당 idx의 priority를 p값으로 바꾸겠다\n",
    "        change = p - self.tree[idx] # 새로운 p와 기존에 idx가 가지고 있던 p의 차이 (p의 변화량)\n",
    "        self.tree[idx] = p # 해당 idx에 새로운 p를 넣고\n",
    "        self._propagate(idx, change) # 부모 노드에 동일하게 p의 변화량을 반영해줘서 자식 노드 값의 합은 부모노드의 값과 같다는 조건을 유지한다.\n",
    "\n",
    "    # get priority and sample\n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        \"\"\"\n",
    "        def _retrieve(self, idx, s):\n",
    "            left = 2 * idx + 1\n",
    "            right = 2 * idx + 2 # left + 1과 동일한 값\n",
    "\n",
    "            if left >= len(self.tree): # left index값이 dataset의 길이보다 크거나 같으면\n",
    "                return idx\n",
    "\n",
    "            if s <= self.tree[left]:\n",
    "                return self._retrieve(left, s)\n",
    "            else: # s > tree[left]\n",
    "                return self._retrieve(right, s - self.tree[left])\n",
    "        \n",
    "        \"\"\"\n",
    "        dataIdx = idx - self.capacity + 1\n",
    "\n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        self.capacity = capacity\n",
    "\n",
    "    # p = |error| + epsilon 계산\n",
    "    def _get_priority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "\n",
    "    def add(self, error, sample):\n",
    "        p = self._get_priority(error) # p 구하고\n",
    "        self.tree.add(p, sample)\n",
    "\n",
    "    # n = batch size (아래에 그렇게 들어감..)\n",
    "    def sample(self, n):\n",
    "        batch = []\n",
    "        idxs = []\n",
    "        segment = self.tree.total() / n # ?\n",
    "        priorities = [] # prioritiy score : p set\n",
    "\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "\n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            priorities.append(p)\n",
    "            batch.append(data)\n",
    "            idxs.append(idx)\n",
    "\n",
    "        sampling_probabilities = priorities / self.tree.total()\n",
    "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
    "        is_weight /= is_weight.max()\n",
    "\n",
    "        return batch, idxs, is_weight\n",
    "\n",
    "    def update(self, idx, error):\n",
    "        p = self._get_priority(error)\n",
    "        self.tree.update(idx, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 60   score: 33.0   memory length: 1452   epsilon: 0.9103059999999857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: divide by zero encountered in power\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cea8a25fb6b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[1;31m# every time step do the training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_entries\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                 \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-cea8a25fb6b1>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[0mmini_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m     \"\"\"\n\u001b[1;32m--> 234\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPISODES = 500\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(state_size, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, action_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# DQN Agent for the Cartpole\n",
    "# it uses Neural Network to approximate q function\n",
    "# and prioritized experience replay memory & target q network\n",
    "class DQNAgent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # if you want to see Cartpole learning, then change to True\n",
    "        self.render = False\n",
    "        self.load_model = False\n",
    "\n",
    "        # get size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # These are hyper parameters for the DQN\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.memory_size = 20000\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.explore_step = 5000\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "\n",
    "        # create prioritized replay memory using SumTree\n",
    "        self.memory = Memory(self.memory_size)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        self.model.apply(self.weights_init)\n",
    "        self.target_model = DQN(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(),\n",
    "                                    lr=self.learning_rate)\n",
    "\n",
    "        # initialize target model\n",
    "        self.update_target_model()\n",
    "\n",
    "        if self.load_model:\n",
    "            self.model = torch.load('save_model/cartpole_dqn')\n",
    "\n",
    "    # weight xavier initialize\n",
    "    def weights_init(self, m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Linear') != -1:\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state)\n",
    "            state = Variable(state).float().cpu()\n",
    "            q_value = self.model(state)\n",
    "            _, action = torch.max(q_value, 1)\n",
    "            return int(action)\n",
    "\n",
    "    # save sample (error,<s,a,r,s'>) to the replay memory\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        target = self.model(Variable(torch.FloatTensor(state))).data\n",
    "        old_val = target[0][action]\n",
    "        target_val = self.target_model(Variable(torch.FloatTensor(next_state))).data\n",
    "        if done:\n",
    "            target[0][action] = reward\n",
    "        else:\n",
    "            target[0][action] = reward + self.discount_factor * torch.max(target_val)\n",
    "\n",
    "        error = abs(old_val - target[0][action])\n",
    "\n",
    "        self.memory.add(error, (state, action, reward, next_state, done))\n",
    "\n",
    "    # pick samples from prioritized replay memory (with batch_size)\n",
    "    def train_model(self):\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "\n",
    "        mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
    "        mini_batch = np.array(mini_batch).transpose()\n",
    "\n",
    "        states = np.vstack(mini_batch[0])\n",
    "        actions = list(mini_batch[1])\n",
    "        rewards = list(mini_batch[2])\n",
    "        next_states = np.vstack(mini_batch[3])\n",
    "        dones = mini_batch[4]\n",
    "\n",
    "        # bool to binary\n",
    "        dones = dones.astype(int)\n",
    "\n",
    "        # Q function of current state\n",
    "        states = torch.Tensor(states)\n",
    "        states = Variable(states).float()\n",
    "        pred = self.model(states)\n",
    "\n",
    "        # one-hot encoding\n",
    "        a = torch.LongTensor(actions).view(-1, 1)\n",
    "\n",
    "        one_hot_action = torch.FloatTensor(self.batch_size, self.action_size).zero_()\n",
    "        one_hot_action.scatter_(1, a, 1)\n",
    "\n",
    "        pred = torch.sum(pred.mul(Variable(one_hot_action)), dim=1)\n",
    "\n",
    "        # Q function of next state\n",
    "        next_states = torch.Tensor(next_states)\n",
    "        next_states = Variable(next_states).float()\n",
    "        next_pred = self.target_model(next_states).data\n",
    "\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        # Q Learning: get maximum Q value at s' from target model\n",
    "        target = rewards + (1 - dones) * self.discount_factor * next_pred.max(1)[0]\n",
    "        target = Variable(target)\n",
    "\n",
    "        # |error| or |delta|를 의미\n",
    "        errors = torch.abs(pred - target).data.numpy()\n",
    "\n",
    "        # update priority\n",
    "        for i in range(self.batch_size):\n",
    "            idx = idxs[i]\n",
    "            self.memory.update(idx, errors[i])\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # MSE Loss function\n",
    "        loss = F.mse_loss(pred, target)\n",
    "        loss.backward()\n",
    "\n",
    "        # and train\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # In case of CartPole-v1, maximum length of episode is 500\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "    model = DQN(state_size, action_size)\n",
    "\n",
    "    agent = DQNAgent(state_size, action_size)\n",
    "    scores, episodes = [], []\n",
    "\n",
    "    for e in range(EPISODES):\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        while not done:\n",
    "            if agent.render:\n",
    "                env.render()\n",
    "\n",
    "            # get action for the current state and go one step in environment\n",
    "            action = agent.get_action(state)\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            # if an action make the episode end, then gives penalty of -100\n",
    "            reward = reward if not done or score == 499 else -10\n",
    "\n",
    "            # save the sample <s, a, r, s'> to the replay memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # every time step do the training\n",
    "            if agent.memory.tree.n_entries >= agent.train_start:\n",
    "                agent.train_model()\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                # every episode update the target model to be same with model\n",
    "                agent.update_target_model()\n",
    "\n",
    "                # every episode, plot the play time\n",
    "                score = score if score == 500 else score + 10\n",
    "                scores.append(score)\n",
    "                episodes.append(e)\n",
    "                plt.plot(episodes, scores, 'b')\n",
    "                plt.savefig(\"./save_graph/cartpole_dqn.png\")\n",
    "                clear_output()\n",
    "                print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                      agent.memory.tree.n_entries, \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "                # if the mean of scores of last 10 episode is bigger than 490\n",
    "                # stop training\n",
    "                if np.mean(scores[-min(10, len(scores)):]) > 490:\n",
    "                    torch.save(agent.model, \"./save_model/cartpole_dqn\")\n",
    "                    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- heap sort를 기반으로한 sum tree방식으로 transitions들을 정렬하였다.\n",
    "- 그러기 위해 기존의 (s,a,r,s')외에 error가 추가로 memory구성에 들어갔다.\n",
    "- error를 구하는 방식은 논문에 나온 그대로(p = |error| + epsilon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
