{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observation은 환경에 대한 정보를 포함한 4-dimension vector로 <br>\n",
    "각각 \n",
    "```[Cart Position(카트의 위치), Cart Velocity(카트의 속도), Pole Angle(막대기의 각도), Pole Velocity At Tip(막대기 끝의 속도)]```을 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # layer 설정\n",
    "import torch.optim as optim # optimizer 설정\n",
    "import torch.nn.functional as F # loss function, activation function 설정\n",
    "import torchvision.transforms as T\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# gpu사용시\n",
    "# 하지만 gpu쓰려고 colab열면 gym.render가 안되지~~\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape :  (4,)\n",
      "# of action :  2\n"
     ]
    }
   ],
   "source": [
    "print(\"state shape : \",env.observation_space.shape)\n",
    "print(\"# of action : \",env.action_space.n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2000\n",
    "env = gym.make('CartPole-v0')\n",
    "env = env.unwrapped\n",
    "N_ACTIONS = env.action_space.n\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "# isinstance : action_space의 sample action의 type이 int인가? True or False return\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network for DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(N_STATES, 24)\n",
    "        self.l1.weight.data.normal_(0,0.1) # weight 초기화\n",
    "        self.l2 = nn.Linear(24,24)\n",
    "        self.l2.weight.data.normal_(0,0.1)\n",
    "        self.out = nn.Linear(24, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0,0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        actions_value = self.out(x)\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get Ready for Training!!\n"
     ]
    }
   ],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        # 먼저 prediction network와 target network 분리\n",
    "        self.pred_net, self.target_net = Net(), Net()\n",
    "        \n",
    "        self.learn_step_counter = 0 # for target updating\n",
    "        self.memory_counter = 0 # for storing memory\n",
    "        # (capacity, 4) ; 4 = (state, action, reward, next_state)\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, N_STATES * 2 + 2))\n",
    "        self.optimizer = torch.optim.Adam(self.pred_net.parameters(), lr=LR)\n",
    "        self.loss_func = nn.MSELoss() # 이러면 알아서 y와 y_hat을 인식하나??\n",
    "        \n",
    "    def choose_action(self, x):\n",
    "        x = torch.unsqueeze(torch.FloatTensor(x), dim=0) # (1, ?)\n",
    "        # 1. greedy \n",
    "        if np.random.uniform() < EPSILON: # EPSILON = 0.9\n",
    "            actions_value = self.pred_net.forward(x)\n",
    "            # [1]은 return값중 argmax를 받겠다는 의미\n",
    "            # max의 return값이 max, argmax 두개가 나옴\n",
    "            # .data까지는 print해주는 output의 변화없고 이걸 numpy로 변환\n",
    "            action = torch.max(actions_value, dim=1)[1].data.numpy()\n",
    "            # 아래가 뭘 의미하는 거지...\n",
    "            action = action[0] if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        \n",
    "        # random\n",
    "        else:\n",
    "            action = np.random.randint(0, N_ACTIONS) # 0,1중에 하나 선택\n",
    "            action = action if ENV_A_SHAPE == 0 else action.reshape(ENV_A_SHAPE)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        # state가 list/array이기때문에, action과 reward를 []로 묶어준다.\n",
    "        # **state가 4가지 정보를 담고있다.\n",
    "        transition = np.hstack([state, [action,reward], next_state])\n",
    "        # replace the old memory with new memory\n",
    "        idx = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[idx,:] = transition\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "    def learn(self):\n",
    "        # TARGET_REPLACE_ITER은 100으로 설정했다.\n",
    "        # 이 말은 즉 target network를 100번에 한번씩 업데이트하겠다는 의미\n",
    "        # pred_net 100번 업데이트하고 나서야 비로소 target_net 1번 업데이트\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            # target의 parameter를 pred_net의 parameter로 대체하겠다는 의미\n",
    "            self.target_net.load_state_dict(self.pred_net.state_dict())\n",
    "            \n",
    "        self.learn_step_counter += 1\n",
    "        \n",
    "        # batch transition sampling\n",
    "        sample_idx = np.random.choice(MEMORY_CAPACITY, BATCH_SIZE)\n",
    "        b_memory = self.memory[sample_idx, :]\n",
    "        b_state = torch.FloatTensor(b_memory[:, :N_STATES])\n",
    "        b_action = torch.LongTensor(b_memory[:, N_STATES : N_STATES+1].astype(int)) # long 이어야함 (Integer)\n",
    "        b_reward = torch.FloatTensor(b_memory[:, N_STATES+1 : N_STATES+2])\n",
    "        b_next_state = torch.FloatTensor(b_memory[:, -N_STATES:])\n",
    "        \n",
    "        q_pred = self.pred_net(b_state).gather(1, b_action) # gather의 의미를 모르겠다..\n",
    "        q_prime = self.target_net(b_next_state).detach() # detach from graph, don't backpropagate(gradient가 흘러들어가 업데이트 되지않도록 예방)\n",
    "        q_target = b_reward + GAMMA * q_prime.max(dim=1)[0].view(BATCH_SIZE,1) # shape (batch, 1)\n",
    "        loss = self.loss_func(q_pred, q_target)\n",
    "        \n",
    "        self.optimizer.zero_grad() # gradient initialize\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "print(\"Get Ready for Training!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting Experiences....\n",
      "Episode : 204, Episode reward : 2.26..\n",
      "Episode : 205, Episode reward : 5.57..\n",
      "Episode : 206, Episode reward : 1.66..\n",
      "Episode : 207, Episode reward : 2.79..\n",
      "Episode : 208, Episode reward : 1.49..\n",
      "Episode : 209, Episode reward : 1.16..\n",
      "Episode : 210, Episode reward : 2.1..\n",
      "Episode : 211, Episode reward : 1.68..\n",
      "Episode : 212, Episode reward : 3.14..\n",
      "Episode : 213, Episode reward : 2.92..\n",
      "Episode : 214, Episode reward : 11.97..\n",
      "Episode : 215, Episode reward : 1.32..\n",
      "Episode : 216, Episode reward : 4.51..\n",
      "Episode : 217, Episode reward : 3.32..\n",
      "Episode : 218, Episode reward : 14.95..\n",
      "Episode : 219, Episode reward : 1.99..\n",
      "Episode : 220, Episode reward : 9.16..\n",
      "Episode : 221, Episode reward : 38.82..\n",
      "Episode : 222, Episode reward : 39.85..\n",
      "Episode : 223, Episode reward : 34.4..\n",
      "Episode : 224, Episode reward : 37.34..\n",
      "Episode : 225, Episode reward : 32.8..\n",
      "Episode : 226, Episode reward : 108.8..\n",
      "Episode : 227, Episode reward : 173.18..\n",
      "Episode : 228, Episode reward : 28.03..\n",
      "Episode : 229, Episode reward : 79.73..\n",
      "Episode : 230, Episode reward : 93.63..\n",
      "Episode : 231, Episode reward : 171.69..\n",
      "Episode : 232, Episode reward : 203.29..\n",
      "Episode : 233, Episode reward : 236.45..\n",
      "Episode : 234, Episode reward : 1315.5..\n",
      "Episode : 235, Episode reward : 433.77..\n",
      "Episode : 236, Episode reward : 202.06..\n",
      "Episode : 237, Episode reward : 520.69..\n",
      "Episode : 238, Episode reward : 273.93..\n",
      "Episode : 239, Episode reward : 371.49..\n",
      "Episode : 240, Episode reward : 335.19..\n",
      "Episode : 241, Episode reward : 296.91..\n",
      "Episode : 242, Episode reward : 294.69..\n",
      "Episode : 243, Episode reward : 166.04..\n",
      "Episode : 244, Episode reward : 326.6..\n",
      "Episode : 245, Episode reward : 107.24..\n",
      "Episode : 246, Episode reward : 266.99..\n",
      "Episode : 247, Episode reward : 99.99..\n",
      "Episode : 248, Episode reward : 206.5..\n",
      "Episode : 249, Episode reward : 301.82..\n",
      "Episode : 250, Episode reward : 410.27..\n",
      "Episode : 251, Episode reward : 358.7..\n",
      "Episode : 252, Episode reward : 992.52..\n",
      "Episode : 253, Episode reward : 191.8..\n",
      "Episode : 254, Episode reward : 153.17..\n",
      "Episode : 255, Episode reward : 191.04..\n",
      "Episode : 256, Episode reward : 169.57..\n",
      "Episode : 257, Episode reward : 292.42..\n",
      "Episode : 258, Episode reward : 173.79..\n",
      "Episode : 259, Episode reward : 250.7..\n",
      "Episode : 260, Episode reward : 392.2..\n",
      "Episode : 261, Episode reward : 1356.48..\n",
      "Episode : 262, Episode reward : 192.64..\n",
      "Episode : 263, Episode reward : 455.08..\n",
      "Episode : 264, Episode reward : 519.37..\n",
      "Episode : 265, Episode reward : 244.45..\n",
      "Episode : 266, Episode reward : 571.54..\n",
      "Episode : 267, Episode reward : 179.41..\n",
      "Episode : 268, Episode reward : 320.85..\n",
      "Episode : 269, Episode reward : 523.26..\n",
      "Episode : 270, Episode reward : 728.94..\n",
      "Episode : 271, Episode reward : 893.37..\n",
      "Episode : 272, Episode reward : 133.39..\n",
      "Episode : 273, Episode reward : 251.43..\n",
      "Episode : 274, Episode reward : 603.44..\n",
      "Episode : 275, Episode reward : 479.71..\n",
      "Episode : 276, Episode reward : 923.36..\n",
      "Episode : 277, Episode reward : 378.63..\n",
      "Episode : 278, Episode reward : 106.18..\n",
      "Episode : 279, Episode reward : 510.96..\n",
      "Episode : 280, Episode reward : 1297.05..\n",
      "Episode : 281, Episode reward : 419.79..\n",
      "Episode : 282, Episode reward : 594.03..\n",
      "Episode : 283, Episode reward : 538.87..\n",
      "Episode : 284, Episode reward : 362.87..\n",
      "Episode : 285, Episode reward : 642.34..\n",
      "Episode : 286, Episode reward : 137.66..\n",
      "Episode : 287, Episode reward : 203.68..\n",
      "Episode : 288, Episode reward : 527.5..\n",
      "Episode : 289, Episode reward : 191.44..\n",
      "Episode : 290, Episode reward : 255.26..\n",
      "Episode : 291, Episode reward : 346.36..\n",
      "Episode : 292, Episode reward : 786.02..\n",
      "Episode : 293, Episode reward : 332.06..\n",
      "Episode : 294, Episode reward : 434.43..\n",
      "Episode : 295, Episode reward : 666.9..\n",
      "Episode : 296, Episode reward : 238.38..\n",
      "Episode : 297, Episode reward : 1250.78..\n",
      "Episode : 298, Episode reward : 182.36..\n",
      "Episode : 299, Episode reward : 189.7..\n",
      "Episode : 300, Episode reward : 554.51..\n",
      "Episode : 301, Episode reward : 378.64..\n",
      "Episode : 302, Episode reward : 185.64..\n",
      "Episode : 303, Episode reward : 411.82..\n",
      "Episode : 304, Episode reward : 454.28..\n",
      "Episode : 305, Episode reward : 391.69..\n",
      "Episode : 306, Episode reward : 241.83..\n",
      "Episode : 307, Episode reward : 361.4..\n",
      "Episode : 308, Episode reward : 159.17..\n",
      "Episode : 309, Episode reward : 362.15..\n",
      "Episode : 310, Episode reward : 878.11..\n",
      "Episode : 311, Episode reward : 646.69..\n",
      "Episode : 312, Episode reward : 271.38..\n",
      "Episode : 313, Episode reward : 661.84..\n",
      "Episode : 314, Episode reward : 286.35..\n",
      "Episode : 315, Episode reward : 142.2..\n",
      "Episode : 316, Episode reward : 460.17..\n",
      "Episode : 317, Episode reward : 160.61..\n",
      "Episode : 318, Episode reward : 1779.59..\n",
      "Episode : 319, Episode reward : 1217.42..\n",
      "Episode : 320, Episode reward : 450.06..\n",
      "Episode : 321, Episode reward : 497.52..\n",
      "Episode : 322, Episode reward : 357.0..\n",
      "Episode : 323, Episode reward : 291.13..\n",
      "Episode : 324, Episode reward : 136.71..\n",
      "Episode : 325, Episode reward : 302.0..\n",
      "Episode : 326, Episode reward : 155.19..\n",
      "Episode : 327, Episode reward : 584.39..\n",
      "Episode : 328, Episode reward : 321.42..\n",
      "Episode : 329, Episode reward : 183.25..\n",
      "Episode : 330, Episode reward : 250.86..\n",
      "Episode : 331, Episode reward : 332.47..\n",
      "Episode : 332, Episode reward : 593.63..\n",
      "Episode : 333, Episode reward : 1948.55..\n",
      "Episode : 334, Episode reward : 258.51..\n",
      "Episode : 335, Episode reward : 126.93..\n",
      "Episode : 336, Episode reward : 537.88..\n",
      "Episode : 337, Episode reward : 978.76..\n",
      "Episode : 338, Episode reward : 558.32..\n",
      "Episode : 339, Episode reward : 763.13..\n",
      "Episode : 340, Episode reward : 915.85..\n",
      "Episode : 341, Episode reward : 295.97..\n",
      "Episode : 342, Episode reward : 192.86..\n",
      "Episode : 343, Episode reward : 455.28..\n",
      "Episode : 344, Episode reward : 352.19..\n",
      "Episode : 345, Episode reward : 239.02..\n",
      "Episode : 346, Episode reward : 252.84..\n",
      "Episode : 347, Episode reward : 300.96..\n",
      "Episode : 348, Episode reward : 294.04..\n",
      "Episode : 349, Episode reward : 906.92..\n",
      "Episode : 350, Episode reward : 662.55..\n",
      "Episode : 351, Episode reward : 925.62..\n",
      "Episode : 352, Episode reward : 202.05..\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCollecting Experiences....\")\n",
    "ep_reward_ls = []\n",
    "for i_episode in range(500):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = dqn.choose_action(state)\n",
    "        \n",
    "        # take action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # reward 수정\n",
    "        x, x_dot, theta, theta_dot = next_state\n",
    "        r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "        r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "        reward = r1 + r2\n",
    "        \n",
    "        dqn.store_transition(state, action, reward, next_state)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        ep_reward_ls.append(episode_reward)\n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "            dqn.learn()\n",
    "            if done:\n",
    "                print(\"Episode : {}, Episode reward : {}..\".format(i_episode, round(episode_reward,2)) )\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 좀 더 안정적으로 학습할 수 없을까?\n",
    "- 넘어지지는 않는데..그대로 화면밖으로 나가버림...\n",
    "- 화면밖으로 나가면(done에 도달하면) penalty를 주는 방식으로 해야할 거 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.FloatTensor([1,2,3])\n",
    "print(a.size())\n",
    "print(torch.unsqueeze(a, 1))\n",
    "print(torch.unsqueeze(a, 1).shape)\n",
    "b = torch.unsqueeze(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(b, 1)[1].data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69, 44, 49, 99, 86, 82, 73, 18, 28, 74])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
