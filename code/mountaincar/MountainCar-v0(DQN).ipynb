{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
      "state shape :  (2,)\n",
      "# of action :  3\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "print(\"state shape : \",env.observation_space.shape)\n",
    "print(\"# of action : \",env.action_space.n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state정보는 `position`, `velocity`\n",
    "- action은 `left','right','nothing(가만히)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameter Setting\n",
    "# 그때그때 생각나면 추가하도록 하자\n",
    "N_STATES = env.observation_space.shape[0] # 2\n",
    "N_ACTIONS = env.action_space.n # 3\n",
    "MEMORY_CAPACITY = 2000\n",
    "learning_rate = 0.01\n",
    "EPSILON = 0.9\n",
    "TARGET_REPLACE_ITER = 100\n",
    "batch_size = 32\n",
    "GAMMA = 0.9\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(N_STATES, 16)\n",
    "        self.l1.weight.data.normal_(0,0.1) # weight 초기화\n",
    "        self.l2 = nn.Linear(16,16)\n",
    "        self.l2.weight.data.normal_(0,0.1)\n",
    "        self.out = nn.Linear(16, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0,0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        action_value = self.out(x)\n",
    "        return action_value # q value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0640, 0.2357, 0.0432]], grad_fn=<AddmmBackward>)\n",
      "(tensor([0.2357], grad_fn=<MaxBackward0>), tensor([1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0640]], grad_fn=<GatherBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 잘 나오는 지 확인\n",
    "# torch.gather의 의미 파악\n",
    "# torch.gather : dim = 1에서 index=0의 값을 가져와라! (아래 예시참고)\n",
    "net = Net()\n",
    "out = net(torch.Tensor([[1,2]]))\n",
    "print(out)\n",
    "print(out.max(dim=1))\n",
    "out.gather(1,torch.LongTensor([[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Agent (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to Go!\n"
     ]
    }
   ],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # target, prediction network 분리\n",
    "        self.target_network, self.prediction_network = Net(), Net()\n",
    "        \n",
    "        self.learn_step_counter = 0 # setting for target update\n",
    "        self.memory_counter = 0 # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, 6))\n",
    "        self.optimizer = optim.Adam(self.prediction_network.parameters(), lr=learning_rate)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "    \n",
    "    # greedy action selection\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        value = self.prediction_network(state)\n",
    "        max_value, arg_max = torch.max(value, 1)\n",
    "        action = arg_max.item()\n",
    "        if np.random.rand(1) >= 0.9: # epslion greedy\n",
    "            action = np.random.choice(range(N_ACTIONS), 1).item()\n",
    "        return action\n",
    "            \n",
    "        return \n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        transition = np.hstack([state, [action, reward], next_state])\n",
    "        # replace the old memory with new memory\n",
    "        idx = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[idx, :] = transition\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_network.load_state_dict(self.prediction_network.state_dict())\n",
    "        \n",
    "        else:\n",
    "            self.learn_step_counter += 1\n",
    "            \n",
    "        # batch transition sampling\n",
    "        sample_idx = np.random.choice(MEMORY_CAPACITY, batch_size)\n",
    "        batch_memory = self.memory[sample_idx, :]\n",
    "        b_state = torch.FloatTensor(batch_memory[:, :N_STATES])\n",
    "        b_action = torch.LongTensor(batch_memory[:, N_STATES:N_STATES+1].astype('int'))\n",
    "        b_reward = torch.FloatTensor(batch_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_next_state = torch.FloatTensor(batch_memory[:, N_STATES+2:])\n",
    "        \n",
    "        q_prediction = self.prediction_network(b_state).gather(dim=1, index=b_action)\n",
    "        q_prime = self.target_network(b_next_state).detach() # don't backward with prediction network\n",
    "        q_target = b_reward + GAMMA * q_prime.max(dim=1)[0].view(batch_size,1)\n",
    "        loss = self.loss_func(q_prediction, q_target) # prediction, target 순서\n",
    "        \n",
    "        self. optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "print(\"Ready to Go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Expriences...\n",
      "Episode : 0, Episode reward : -200.0..\n",
      "Episode : 1, Episode reward : -200.0..\n",
      "Episode : 2, Episode reward : -200.0..\n",
      "Episode : 3, Episode reward : -200.0..\n",
      "Episode : 4, Episode reward : -200.0..\n",
      "Episode : 5, Episode reward : -200.0..\n",
      "Episode : 6, Episode reward : -200.0..\n",
      "Episode : 7, Episode reward : -200.0..\n",
      "Episode : 8, Episode reward : -200.0..\n",
      "Episode : 9, Episode reward : -200.0..\n",
      "Episode : 10, Episode reward : -200.0..\n",
      "Episode : 11, Episode reward : -200.0..\n",
      "Episode : 12, Episode reward : -200.0..\n",
      "Episode : 13, Episode reward : -200.0..\n",
      "Episode : 14, Episode reward : -200.0..\n",
      "Episode : 15, Episode reward : -200.0..\n",
      "Episode : 16, Episode reward : -200.0..\n",
      "Episode : 17, Episode reward : -200.0..\n",
      "Episode : 18, Episode reward : -200.0..\n",
      "Episode : 19, Episode reward : -200.0..\n",
      "Episode : 20, Episode reward : -200.0..\n",
      "Episode : 21, Episode reward : -200.0..\n",
      "Episode : 22, Episode reward : -200.0..\n",
      "Episode : 23, Episode reward : -200.0..\n",
      "Episode : 24, Episode reward : -200.0..\n",
      "Episode : 25, Episode reward : -200.0..\n",
      "Episode : 26, Episode reward : -200.0..\n",
      "Episode : 27, Episode reward : -200.0..\n",
      "Episode : 28, Episode reward : -200.0..\n",
      "Episode : 29, Episode reward : -200.0..\n",
      "Episode : 30, Episode reward : -200.0..\n",
      "Episode : 31, Episode reward : -200.0..\n",
      "Episode : 32, Episode reward : -200.0..\n",
      "Episode : 33, Episode reward : -200.0..\n",
      "Episode : 34, Episode reward : -200.0..\n",
      "Episode : 35, Episode reward : -200.0..\n",
      "Episode : 36, Episode reward : -200.0..\n",
      "Episode : 37, Episode reward : -200.0..\n",
      "Episode : 38, Episode reward : -200.0..\n",
      "Episode : 39, Episode reward : -200.0..\n",
      "Episode : 40, Episode reward : -200.0..\n",
      "Episode : 41, Episode reward : -200.0..\n",
      "Episode : 42, Episode reward : -200.0..\n",
      "Episode : 43, Episode reward : -200.0..\n",
      "Episode : 44, Episode reward : -200.0..\n",
      "Episode : 45, Episode reward : -200.0..\n",
      "Episode : 46, Episode reward : -200.0..\n",
      "Episode : 47, Episode reward : -200.0..\n",
      "Episode : 48, Episode reward : -200.0..\n",
      "Episode : 49, Episode reward : -200.0..\n",
      "Episode : 50, Episode reward : -200.0..\n",
      "Episode : 51, Episode reward : -200.0..\n",
      "Episode : 52, Episode reward : -200.0..\n",
      "Episode : 53, Episode reward : -200.0..\n",
      "Episode : 54, Episode reward : -200.0..\n",
      "Episode : 55, Episode reward : -200.0..\n",
      "Episode : 56, Episode reward : -200.0..\n",
      "Episode : 57, Episode reward : -200.0..\n",
      "Episode : 58, Episode reward : -200.0..\n",
      "Episode : 59, Episode reward : -200.0..\n",
      "Episode : 60, Episode reward : -200.0..\n",
      "Episode : 61, Episode reward : -200.0..\n",
      "Episode : 62, Episode reward : -200.0..\n",
      "Episode : 63, Episode reward : -200.0..\n",
      "Episode : 64, Episode reward : -200.0..\n",
      "Episode : 65, Episode reward : -200.0..\n",
      "Episode : 66, Episode reward : -200.0..\n",
      "Episode : 67, Episode reward : -200.0..\n",
      "Episode : 68, Episode reward : -200.0..\n",
      "Episode : 69, Episode reward : -200.0..\n",
      "Episode : 70, Episode reward : -200.0..\n",
      "Episode : 71, Episode reward : -200.0..\n",
      "Episode : 72, Episode reward : -200.0..\n",
      "Episode : 73, Episode reward : -200.0..\n",
      "Episode : 74, Episode reward : -200.0..\n",
      "Episode : 75, Episode reward : -200.0..\n",
      "Episode : 76, Episode reward : -200.0..\n",
      "Episode : 77, Episode reward : -200.0..\n",
      "Episode : 78, Episode reward : -200.0..\n",
      "Episode : 79, Episode reward : -200.0..\n",
      "Episode : 80, Episode reward : -200.0..\n",
      "Episode : 81, Episode reward : -200.0..\n",
      "Episode : 82, Episode reward : -200.0..\n",
      "Episode : 83, Episode reward : -200.0..\n",
      "Episode : 84, Episode reward : -200.0..\n",
      "Episode : 85, Episode reward : -200.0..\n",
      "Episode : 86, Episode reward : -200.0..\n",
      "Episode : 87, Episode reward : -200.0..\n",
      "Episode : 88, Episode reward : -200.0..\n",
      "Episode : 89, Episode reward : -200.0..\n",
      "Episode : 90, Episode reward : -200.0..\n",
      "Episode : 91, Episode reward : -200.0..\n",
      "Episode : 92, Episode reward : -200.0..\n",
      "Episode : 93, Episode reward : -200.0..\n",
      "Episode : 94, Episode reward : -200.0..\n",
      "Episode : 95, Episode reward : -200.0..\n",
      "Episode : 96, Episode reward : -200.0..\n",
      "Episode : 97, Episode reward : -200.0..\n",
      "Episode : 98, Episode reward : -200.0..\n",
      "Episode : 99, Episode reward : -200.0..\n",
      "Episode : 100, Episode reward : -200.0..\n",
      "Episode : 101, Episode reward : -200.0..\n",
      "Episode : 102, Episode reward : -200.0..\n",
      "Episode : 103, Episode reward : -200.0..\n",
      "Episode : 104, Episode reward : -200.0..\n",
      "Episode : 105, Episode reward : -200.0..\n",
      "Episode : 106, Episode reward : -200.0..\n",
      "Episode : 107, Episode reward : -200.0..\n",
      "Episode : 108, Episode reward : -200.0..\n",
      "Episode : 109, Episode reward : -200.0..\n",
      "Episode : 110, Episode reward : -200.0..\n",
      "Episode : 111, Episode reward : -200.0..\n",
      "Episode : 112, Episode reward : -200.0..\n",
      "Episode : 113, Episode reward : -200.0..\n",
      "Episode : 114, Episode reward : -200.0..\n",
      "Episode : 115, Episode reward : -200.0..\n",
      "Episode : 116, Episode reward : -200.0..\n",
      "Episode : 117, Episode reward : -200.0..\n",
      "Episode : 118, Episode reward : -200.0..\n",
      "Episode : 119, Episode reward : -200.0..\n",
      "Episode : 120, Episode reward : -200.0..\n",
      "Episode : 121, Episode reward : -200.0..\n",
      "Episode : 122, Episode reward : -200.0..\n",
      "Episode : 123, Episode reward : -200.0..\n",
      "Episode : 124, Episode reward : -200.0..\n",
      "Episode : 125, Episode reward : -200.0..\n",
      "Episode : 126, Episode reward : -200.0..\n",
      "Episode : 127, Episode reward : -200.0..\n",
      "Episode : 128, Episode reward : -200.0..\n",
      "Episode : 129, Episode reward : -200.0..\n",
      "Episode : 130, Episode reward : -200.0..\n",
      "Episode : 131, Episode reward : -200.0..\n",
      "Episode : 132, Episode reward : -200.0..\n",
      "Episode : 133, Episode reward : -200.0..\n",
      "Episode : 134, Episode reward : -200.0..\n",
      "Episode : 135, Episode reward : -200.0..\n",
      "Episode : 136, Episode reward : -200.0..\n",
      "Episode : 137, Episode reward : -200.0..\n",
      "Episode : 138, Episode reward : -200.0..\n",
      "Episode : 139, Episode reward : -200.0..\n",
      "Episode : 140, Episode reward : -200.0..\n",
      "Episode : 141, Episode reward : -200.0..\n",
      "Episode : 142, Episode reward : -200.0..\n",
      "Episode : 143, Episode reward : -200.0..\n",
      "Episode : 144, Episode reward : -200.0..\n",
      "Episode : 145, Episode reward : -200.0..\n",
      "Episode : 146, Episode reward : -200.0..\n",
      "Episode : 147, Episode reward : -200.0..\n",
      "Episode : 148, Episode reward : -200.0..\n",
      "Episode : 149, Episode reward : -200.0..\n",
      "Episode : 150, Episode reward : -200.0..\n",
      "Episode : 151, Episode reward : -200.0..\n",
      "Episode : 152, Episode reward : -200.0..\n",
      "Episode : 153, Episode reward : -200.0..\n",
      "Episode : 154, Episode reward : -200.0..\n",
      "Episode : 155, Episode reward : -200.0..\n",
      "Episode : 156, Episode reward : -200.0..\n",
      "Episode : 157, Episode reward : -200.0..\n",
      "Episode : 158, Episode reward : -200.0..\n",
      "Episode : 159, Episode reward : -200.0..\n",
      "Episode : 160, Episode reward : -200.0..\n",
      "Episode : 161, Episode reward : -200.0..\n",
      "Episode : 162, Episode reward : -200.0..\n",
      "Episode : 163, Episode reward : -200.0..\n",
      "Episode : 164, Episode reward : -200.0..\n",
      "Episode : 165, Episode reward : -200.0..\n",
      "Episode : 166, Episode reward : -200.0..\n",
      "Episode : 167, Episode reward : -200.0..\n",
      "Episode : 168, Episode reward : -200.0..\n",
      "Episode : 169, Episode reward : -200.0..\n",
      "Episode : 170, Episode reward : -200.0..\n",
      "Episode : 171, Episode reward : -200.0..\n",
      "Episode : 172, Episode reward : -200.0..\n",
      "Episode : 173, Episode reward : -200.0..\n",
      "Episode : 174, Episode reward : -200.0..\n",
      "Episode : 175, Episode reward : -200.0..\n",
      "Episode : 176, Episode reward : -200.0..\n",
      "Episode : 177, Episode reward : -200.0..\n",
      "Episode : 178, Episode reward : -200.0..\n",
      "Episode : 179, Episode reward : -200.0..\n",
      "Episode : 180, Episode reward : -200.0..\n",
      "Episode : 181, Episode reward : -200.0..\n",
      "Episode : 182, Episode reward : -200.0..\n",
      "Episode : 183, Episode reward : -200.0..\n",
      "Episode : 184, Episode reward : -200.0..\n",
      "Episode : 185, Episode reward : -200.0..\n",
      "Episode : 186, Episode reward : -200.0..\n",
      "Episode : 187, Episode reward : -200.0..\n",
      "Episode : 188, Episode reward : -200.0..\n",
      "Episode : 189, Episode reward : -200.0..\n",
      "Episode : 190, Episode reward : -200.0..\n",
      "Episode : 191, Episode reward : -200.0..\n",
      "Episode : 192, Episode reward : -200.0..\n",
      "Episode : 193, Episode reward : -200.0..\n",
      "Episode : 194, Episode reward : -200.0..\n",
      "Episode : 195, Episode reward : -200.0..\n",
      "Episode : 196, Episode reward : -200.0..\n",
      "Episode : 197, Episode reward : -200.0..\n",
      "Episode : 198, Episode reward : -200.0..\n",
      "Episode : 199, Episode reward : -200.0..\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecting Expriences...\")\n",
    "ep_reward_ls = []\n",
    "for i_episode in range(200):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = dqn.select_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        dqn.store_transition(state, action, reward, next_state)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "#             print(\"Learning Start!\")\n",
    "            dqn.learn()\n",
    "            \n",
    "            if done:\n",
    "                print(\"Episode : {}, Episode reward : {}..\".format(i_episode, round(episode_reward,2)) )\n",
    "                ep_reward_ls.append(episode_reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
