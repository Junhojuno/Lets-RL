{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n",
      "state shape :  (2,)\n",
      "# of action :  3\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "print(\"state shape : \",env.observation_space.shape)\n",
    "print(\"# of action : \",env.action_space.n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- state정보는 `position`, `velocity`\n",
    "- action은 `left','right','nothing(가만히)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameter Setting\n",
    "# 그때그때 생각나면 추가하도록 하자\n",
    "N_STATES = env.observation_space.shape[0] # 2\n",
    "N_ACTIONS = env.action_space.n # 3\n",
    "MEMORY_CAPACITY = 20000\n",
    "learning_rate = 0.01\n",
    "EPSILON = 0.9\n",
    "TARGET_REPLACE_ITER = 100\n",
    "batch_size = 32\n",
    "GAMMA = 0.9\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(N_STATES, 16)\n",
    "        self.l1.weight.data.normal_(0,0.1) # weight 초기화\n",
    "        self.l2 = nn.Linear(16,16)\n",
    "        self.l2.weight.data.normal_(0,0.1)\n",
    "        self.out = nn.Linear(16, N_ACTIONS)\n",
    "        self.out.weight.data.normal_(0,0.1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        action_value = self.out(x)\n",
    "        return action_value # q value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0826,  0.0600,  0.1756]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 잘 나오는 지 확인\n",
    "# torch.gather의 의미 파악\n",
    "# torch.gather : dim = 1에서 index=0의 값을 가져와라! (아래 예시참고)\n",
    "net = Net()\n",
    "out = net(torch.Tensor([[1,2]]))\n",
    "print(out)\n",
    "# print(out.max(dim=1))\n",
    "# out.gather(1,torch.LongTensor([[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Agent (DQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to Go!\n"
     ]
    }
   ],
   "source": [
    "class DoubleDQNAgent:\n",
    "    def __init__(self):\n",
    "        # target, prediction network 분리\n",
    "        self.q_new, self.q_old = Net(), Net()\n",
    "        \n",
    "        self.learn_step_counter = 0 # setting for target update\n",
    "        self.memory_counter = 0 # for storing memory\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, 6))\n",
    "        self.optimizer = optim.Adam(self.q_new.parameters(), lr=learning_rate)\n",
    "        self.loss_func = nn.MSELoss()\n",
    "    \n",
    "    # greedy action selection\n",
    "    def select_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float).unsqueeze(0)\n",
    "        value = self.q_new(state)\n",
    "        max_value, arg_max = torch.max(value, 1)\n",
    "        action = arg_max.item()\n",
    "        if np.random.rand(1) >= 0.9: # epslion greedy\n",
    "            action = np.random.choice(range(N_ACTIONS), 1).item()\n",
    "        return action\n",
    "            \n",
    "        return \n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state):\n",
    "        transition = np.hstack([state, [action, reward], next_state])\n",
    "        # replace the old memory with new memory\n",
    "        idx = self.memory_counter % MEMORY_CAPACITY\n",
    "        self.memory[idx, :] = transition\n",
    "        self.memory_counter += 1\n",
    "        \n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.q_old.load_state_dict(self.q_new.state_dict())\n",
    "        \n",
    "        else:\n",
    "            self.learn_step_counter += 1\n",
    "            \n",
    "        # batch transition sampling\n",
    "        sample_idx = np.random.choice(MEMORY_CAPACITY, batch_size)\n",
    "        batch_memory = self.memory[sample_idx, :]\n",
    "        b_state = torch.FloatTensor(batch_memory[:, :N_STATES])\n",
    "        b_action = torch.LongTensor(batch_memory[:, N_STATES:N_STATES+1].astype('int'))\n",
    "        b_reward = torch.FloatTensor(batch_memory[:, N_STATES+1:N_STATES+2])\n",
    "        b_next_state = torch.FloatTensor(batch_memory[:, N_STATES+2:])\n",
    "        \n",
    "        # prediction = r + gamma*Q(s', argmax(Q(s',a)))\n",
    "        # Target을 action selection과 evaluation으로 나눈다.\n",
    "        q_old_value = self.q_old(b_next_state).detach()\n",
    "        argmax_action = q_old_value.max(dim=1)[1].view(batch_size,1)\n",
    "        q_new_value = b_reward + GAMMA * self.q_new(b_state).gather(dim=1, index=argmax_action)\n",
    "        loss = self.loss_func(q_new_value, q_old_value) # prediction, target 순서\n",
    "        \n",
    "        self. optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "print(\"Ready to Go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "dqn = DoubleDQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 499, Episode reward : -200.0..\n"
     ]
    }
   ],
   "source": [
    "print(\"Collecting Expriences...\")\n",
    "ep_reward_ls = []\n",
    "for i_episode in range(500):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    while True:\n",
    "        env.render()\n",
    "        action = dqn.select_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        dqn.store_transition(state, action, reward, next_state)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        \n",
    "        if dqn.memory_counter > MEMORY_CAPACITY:\n",
    "#             print(\"Learning Start!\")\n",
    "            dqn.learn()\n",
    "            \n",
    "            if done:\n",
    "                clear_output()\n",
    "                print(\"Episode : {}, Episode reward : {}..\".format(i_episode, round(episode_reward,2)) )\n",
    "                ep_reward_ls.append(episode_reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
